{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as pretrained\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = (IMG_SIZE**2)*3\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []\n",
    "\n",
    "for label in os.listdir('./JHMDB_video/ReCompress_Videos/'):\n",
    "    labels.append(label)\n",
    "    for video in os.listdir(f'./JHMDB_video/ReCompress_Videos/{label}'):\n",
    "        if video.endswith('.avi'):\n",
    "            data.append((label, f'./JHMDB_video/ReCompress_Videos/{label}/{video}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = pretrained.resnet50(pretrained=True)\n",
    "\n",
    "def get_embedding(module, input, output):\n",
    "  buffer.append(output)\n",
    "\n",
    "resnet50.avgpool.register_forward_hook(get_embedding)\n",
    "\n",
    "t = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                0.229, 0.224, 0.255]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_video():\n",
    "    data_list = []\n",
    "    for i, (label, path) in enumerate(tqdm(data)):\n",
    "        frames = load_video(path, max_frames=MAX_SEQ_LENGTH)\n",
    "        frames = frames.reshape(frames.shape[0], -1)\n",
    "        frames = frames/255\n",
    "        data_list.append((label, frames))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:07<00:00, 116.31it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = prepare_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.data[idx][1], dtype=torch.float32),\n",
    "            torch.tensor([1 if self.data[idx][0] == key else 0 for key in labels_dict.keys()], dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VideoDataset(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, embedding_dim, n_hidden):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.n_hidden = n_hidden\n",
    "\n",
    "    self.wx = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "    self.wh = nn.Linear(self.n_hidden, self.embedding_dim)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, X, h):\n",
    "    out1 = self.wx(X)\n",
    "    out2 = self.wh(h)\n",
    "    a = self.sigmoid(out1+out2)\n",
    "    \n",
    "    return torch.mul(a,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EleAttG_GRU(nn.Module):\n",
    "  def __init__(self, embedding_dim, n_hidden=128, n_classes=None):\n",
    "    super().__init__()\n",
    "  \n",
    "    assert n_classes is not None\n",
    "    \n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_classes = n_classes\n",
    "\n",
    "    self.attention = Attention(self.embedding_dim, self.n_hidden)\n",
    "    self.grucell = nn.GRUCell(self.embedding_dim, self.n_hidden)\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(self.n_hidden, self.n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.n_hidden, self.n_classes),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, X):\n",
    "    '''\n",
    "      x = batch_size * frames * embedding_dim\n",
    "\n",
    "    '''\n",
    "    h = torch.zeros(X.shape[0], self.n_hidden).to(device)\n",
    "    for i in range(X.shape[1]):\n",
    "      X[:,i,:] = self.attention(X[:,i,:].clone(),h)\n",
    "      h = self.grucell(X[:,i,:].clone(), h)\n",
    "\n",
    "    return self.fc(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EleAttG_GRU(12288, 256, len(labels)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for i, (X,y) in enumerate(tqdm(training_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f54ec556731191ae34ffecbd12704bb8f2e6a5cabce98c16cdcccd50acdfe5bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
